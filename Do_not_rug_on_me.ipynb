{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ydchen17/RugPullDetection/blob/main/Do_not_rug_on_me.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esHn1secYkf8"
      },
      "source": [
        "# Prep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "M8q2SAk68DbS",
        "outputId": "8c136695-5b49-4cfc-b6fd-995522245ae2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting jsonschema==3.2.0\n",
            "  Using cached jsonschema-3.2.0-py2.py3-none-any.whl (56 kB)\n",
            "Collecting web3\n",
            "  Using cached web3-6.15.1-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting pycryptodome\n",
            "  Using cached pycryptodome-3.20.0-cp35-abi3-win_amd64.whl.metadata (3.4 kB)\n",
            "Collecting attrs>=17.4.0 (from jsonschema==3.2.0)\n",
            "  Using cached attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting pyrsistent>=0.14.0 (from jsonschema==3.2.0)\n",
            "  Using cached pyrsistent-0.20.0-cp39-cp39-win_amd64.whl.metadata (976 bytes)\n",
            "Collecting setuptools (from jsonschema==3.2.0)\n",
            "  Using cached setuptools-69.1.0-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting six>=1.11.0 (from jsonschema==3.2.0)\n",
            "  Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
            "Collecting aiohttp>=3.7.4.post0 (from web3)\n",
            "  Using cached aiohttp-3.9.3-cp39-cp39-win_amd64.whl.metadata (7.6 kB)\n",
            "Collecting eth-abi>=4.0.0 (from web3)\n",
            "  Using cached eth_abi-5.0.0-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting eth-account>=0.8.0 (from web3)\n",
            "  Using cached eth_account-0.11.0-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting eth-hash>=0.5.1 (from eth-hash[pycryptodome]>=0.5.1->web3)\n",
            "  Using cached eth_hash-0.6.0-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting eth-typing>=3.0.0 (from web3)\n",
            "  Using cached eth_typing-4.0.0-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting eth-utils>=2.1.0 (from web3)\n",
            "  Using cached eth_utils-3.0.0-py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting hexbytes<0.4.0,>=0.1.0 (from web3)\n",
            "  Using cached hexbytes-0.3.1-py3-none-any.whl.metadata (5.5 kB)\n",
            "INFO: pip is looking at multiple versions of web3 to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting web3\n",
            "  Using cached web3-6.15.0-py3-none-any.whl.metadata (4.9 kB)\n",
            "  Using cached web3-6.14.0-py3-none-any.whl.metadata (4.9 kB)\n",
            "  Using cached web3-6.13.0-py3-none-any.whl.metadata (4.9 kB)\n",
            "  Using cached web3-6.12.0-py3-none-any.whl.metadata (4.9 kB)\n",
            "  Using cached web3-6.11.4-py3-none-any.whl.metadata (4.9 kB)\n",
            "  Using cached web3-6.11.3-py3-none-any.whl.metadata (4.9 kB)\n",
            "  Using cached web3-6.11.2-py3-none-any.whl.metadata (4.9 kB)\n",
            "INFO: pip is still looking at multiple versions of web3 to determine which version is compatible with other requirements. This could take a while.\n",
            "  Using cached web3-6.11.1-py3-none-any.whl.metadata (4.9 kB)\n",
            "  Using cached web3-6.11.0-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting hexbytes>=0.1.0 (from web3)\n",
            "  Using cached hexbytes-1.0.0-py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting web3\n",
            "  Using cached web3-6.10.0-py3-none-any.whl.metadata (4.9 kB)\n",
            "  Using cached web3-6.9.0-py3-none-any.whl.metadata (4.9 kB)\n",
            "  Using cached web3-6.8.0-py3-none-any.whl.metadata (5.0 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Using cached web3-6.7.0-py3-none-any.whl.metadata (5.0 kB)\n",
            "  Using cached web3-6.6.1-py3-none-any.whl.metadata (5.0 kB)\n",
            "  Using cached web3-6.5.0-py3-none-any.whl.metadata (5.0 kB)\n",
            "  Using cached web3-6.4.0-py3-none-any.whl.metadata (5.1 kB)\n",
            "  Using cached web3-6.3.0-py3-none-any.whl.metadata (5.1 kB)\n",
            "  Using cached web3-6.2.0-py3-none-any.whl.metadata (5.1 kB)\n",
            "  Using cached web3-6.1.0-py3-none-any.whl.metadata (5.1 kB)\n",
            "  Using cached web3-6.0.0-py3-none-any.whl.metadata (6.0 kB)\n",
            "Collecting parsimonious==0.9.0 (from web3)\n",
            "  Using cached parsimonious-0.9.0-py3-none-any.whl\n",
            "Collecting web3\n",
            "  Using cached web3-5.31.4-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting eth-abi<3.0.0,>=2.2.0 (from web3)\n",
            "  Using cached eth_abi-2.2.0-py3-none-any.whl (28 kB)\n",
            "Collecting eth-account<0.6.0,>=0.5.9 (from web3)\n",
            "  Using cached eth_account-0.5.9-py3-none-any.whl (101 kB)\n",
            "Collecting eth-rlp<0.3 (from web3)\n",
            "  Using cached eth_rlp-0.2.1-py3-none-any.whl (5.0 kB)\n",
            "Collecting eth-typing<3.0.0,>=2.0.0 (from web3)\n",
            "  Using cached eth_typing-2.3.0-py3-none-any.whl (6.2 kB)\n",
            "Collecting eth-utils<2.0.0,>=1.9.5 (from web3)\n",
            "  Using cached eth_utils-1.10.0-py3-none-any.whl (24 kB)\n",
            "Collecting ipfshttpclient==0.8.0a2 (from web3)\n",
            "  Using cached ipfshttpclient-0.8.0a2-py3-none-any.whl (82 kB)\n",
            "Collecting lru-dict<2.0.0,>=1.1.6 (from web3)\n",
            "  Using cached lru_dict-1.3.0-cp39-cp39-win_amd64.whl.metadata (4.7 kB)\n",
            "Collecting protobuf==3.19.5 (from web3)\n",
            "  Using cached protobuf-3.19.5-cp39-cp39-win_amd64.whl (895 kB)\n",
            "Collecting requests<3.0.0,>=2.16.0 (from web3)\n",
            "  Using cached requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting websockets<10,>=9.1 (from web3)\n",
            "  Using cached websockets-9.1-cp39-cp39-win_amd64.whl (90 kB)\n",
            "Collecting pywin32>=223 (from web3)\n",
            "  Using cached pywin32-306-cp39-cp39-win_amd64.whl.metadata (6.4 kB)\n",
            "Collecting multiaddr>=0.0.7 (from ipfshttpclient==0.8.0a2->web3)\n",
            "  Using cached multiaddr-0.0.9-py2.py3-none-any.whl (16 kB)\n",
            "Collecting aiosignal>=1.1.2 (from aiohttp>=3.7.4.post0->web3)\n",
            "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting frozenlist>=1.1.1 (from aiohttp>=3.7.4.post0->web3)\n",
            "  Using cached frozenlist-1.4.1-cp39-cp39-win_amd64.whl.metadata (12 kB)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp>=3.7.4.post0->web3)\n",
            "  Using cached multidict-6.0.5-cp39-cp39-win_amd64.whl.metadata (4.3 kB)\n",
            "Collecting yarl<2.0,>=1.0 (from aiohttp>=3.7.4.post0->web3)\n",
            "  Using cached yarl-1.9.4-cp39-cp39-win_amd64.whl.metadata (32 kB)\n",
            "Collecting async-timeout<5.0,>=4.0 (from aiohttp>=3.7.4.post0->web3)\n",
            "  Using cached async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting parsimonious<0.9.0,>=0.8.0 (from eth-abi<3.0.0,>=2.2.0->web3)\n",
            "  Using cached parsimonious-0.8.1-py3-none-any.whl\n",
            "Collecting bitarray<3,>=1.2.1 (from eth-account<0.6.0,>=0.5.9->web3)\n",
            "  Using cached bitarray-2.9.2-cp39-cp39-win_amd64.whl.metadata (35 kB)\n",
            "Collecting eth-keyfile<0.6.0,>=0.5.0 (from eth-account<0.6.0,>=0.5.9->web3)\n",
            "  Using cached eth_keyfile-0.5.1-py3-none-any.whl (8.3 kB)\n",
            "Collecting eth-keys<0.4.0,>=0.3.4 (from eth-account<0.6.0,>=0.5.9->web3)\n",
            "  Using cached eth_keys-0.3.4-py3-none-any.whl (21 kB)\n",
            "Collecting rlp<3,>=1.0.0 (from eth-account<0.6.0,>=0.5.9->web3)\n",
            "  Using cached rlp-2.0.1-py2.py3-none-any.whl (20 kB)\n",
            "INFO: pip is looking at multiple versions of eth-utils to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting eth-utils<2.0.0,>=1.9.5 (from web3)\n",
            "  Using cached eth_utils-1.9.5-py3-none-any.whl (23 kB)\n",
            "Collecting cytoolz<1.0.0,>=0.10.1 (from eth-utils<2.0.0,>=1.9.5->web3)\n",
            "  Using cached cytoolz-0.12.3-cp39-cp39-win_amd64.whl.metadata (4.7 kB)\n",
            "Collecting charset-normalizer<4,>=2 (from requests<3.0.0,>=2.16.0->web3)\n",
            "  Using cached charset_normalizer-3.3.2-cp39-cp39-win_amd64.whl.metadata (34 kB)\n",
            "Collecting idna<4,>=2.5 (from requests<3.0.0,>=2.16.0->web3)\n",
            "  Using cached idna-3.6-py3-none-any.whl.metadata (9.9 kB)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests<3.0.0,>=2.16.0->web3)\n",
            "  Using cached urllib3-2.2.1-py3-none-any.whl.metadata (6.4 kB)\n",
            "Collecting certifi>=2017.4.17 (from requests<3.0.0,>=2.16.0->web3)\n",
            "  Using cached certifi-2024.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting toolz>=0.8.0 (from cytoolz<1.0.0,>=0.10.1->eth-utils<2.0.0,>=1.9.5->web3)\n",
            "  Using cached toolz-0.12.1-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting varint (from multiaddr>=0.0.7->ipfshttpclient==0.8.0a2->web3)\n",
            "  Using cached varint-1.0.2-py3-none-any.whl\n",
            "Collecting base58 (from multiaddr>=0.0.7->ipfshttpclient==0.8.0a2->web3)\n",
            "  Using cached base58-2.1.1-py3-none-any.whl (5.6 kB)\n",
            "Collecting netaddr (from multiaddr>=0.0.7->ipfshttpclient==0.8.0a2->web3)\n",
            "  Using cached netaddr-1.2.1-py3-none-any.whl.metadata (5.0 kB)\n",
            "Using cached web3-5.31.4-py3-none-any.whl (502 kB)\n",
            "Using cached pycryptodome-3.20.0-cp35-abi3-win_amd64.whl (1.8 MB)\n",
            "Using cached aiohttp-3.9.3-cp39-cp39-win_amd64.whl (366 kB)\n",
            "Using cached attrs-23.2.0-py3-none-any.whl (60 kB)\n",
            "Using cached eth_hash-0.6.0-py3-none-any.whl (8.6 kB)\n",
            "Using cached hexbytes-0.3.1-py3-none-any.whl (5.9 kB)\n",
            "Using cached lru_dict-1.3.0-cp39-cp39-win_amd64.whl (13 kB)\n",
            "Using cached pyrsistent-0.20.0-cp39-cp39-win_amd64.whl (63 kB)\n",
            "Using cached pywin32-306-cp39-cp39-win_amd64.whl (9.3 MB)\n",
            "Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
            "Using cached setuptools-69.1.0-py3-none-any.whl (819 kB)\n",
            "Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
            "Using cached bitarray-2.9.2-cp39-cp39-win_amd64.whl (126 kB)\n",
            "Using cached certifi-2024.2.2-py3-none-any.whl (163 kB)\n",
            "Using cached charset_normalizer-3.3.2-cp39-cp39-win_amd64.whl (100 kB)\n",
            "Using cached cytoolz-0.12.3-cp39-cp39-win_amd64.whl (364 kB)\n",
            "Using cached frozenlist-1.4.1-cp39-cp39-win_amd64.whl (50 kB)\n",
            "Using cached idna-3.6-py3-none-any.whl (61 kB)\n",
            "Using cached multidict-6.0.5-cp39-cp39-win_amd64.whl (28 kB)\n",
            "Using cached urllib3-2.2.1-py3-none-any.whl (121 kB)\n",
            "Using cached yarl-1.9.4-cp39-cp39-win_amd64.whl (76 kB)\n",
            "Using cached toolz-0.12.1-py3-none-any.whl (56 kB)\n",
            "Using cached netaddr-1.2.1-py3-none-any.whl (2.3 MB)\n",
            "Installing collected packages: varint, pywin32, bitarray, websockets, urllib3, toolz, six, setuptools, pyrsistent, pycryptodome, protobuf, netaddr, multidict, lru-dict, idna, hexbytes, frozenlist, eth-typing, eth-hash, charset-normalizer, certifi, base58, attrs, async-timeout, yarl, requests, parsimonious, multiaddr, jsonschema, cytoolz, aiosignal, ipfshttpclient, eth-utils, aiohttp, rlp, eth-keys, eth-abi, eth-rlp, eth-keyfile, eth-account, web3\n",
            "  Attempting uninstall: varint\n",
            "    Found existing installation: varint 1.0.2\n",
            "    Uninstalling varint-1.0.2:\n",
            "      Successfully uninstalled varint-1.0.2\n",
            "  Attempting uninstall: pywin32\n",
            "    Found existing installation: pywin32 306\n",
            "    Uninstalling pywin32-306:\n",
            "      Successfully uninstalled pywin32-306\n",
            "  Attempting uninstall: bitarray\n",
            "    Found existing installation: bitarray 2.9.2\n",
            "    Uninstalling bitarray-2.9.2:\n",
            "      Successfully uninstalled bitarray-2.9.2\n",
            "  Attempting uninstall: websockets\n",
            "    Found existing installation: websockets 9.1\n",
            "    Uninstalling websockets-9.1:\n",
            "      Successfully uninstalled websockets-9.1\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.2.1\n",
            "    Uninstalling urllib3-2.2.1:\n",
            "      Successfully uninstalled urllib3-2.2.1\n",
            "  Attempting uninstall: toolz\n",
            "    Found existing installation: toolz 0.12.1\n",
            "    Uninstalling toolz-0.12.1:\n",
            "      Successfully uninstalled toolz-0.12.1\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.16.0\n",
            "    Uninstalling six-1.16.0:\n",
            "      Successfully uninstalled six-1.16.0\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 69.1.0\n",
            "    Uninstalling setuptools-69.1.0:\n",
            "      Successfully uninstalled setuptools-69.1.0\n",
            "  Attempting uninstall: pyrsistent\n",
            "    Found existing installation: pyrsistent 0.20.0\n",
            "    Uninstalling pyrsistent-0.20.0:\n",
            "      Successfully uninstalled pyrsistent-0.20.0\n",
            "  Attempting uninstall: pycryptodome\n",
            "    Found existing installation: pycryptodome 3.20.0\n",
            "    Uninstalling pycryptodome-3.20.0:\n",
            "      Successfully uninstalled pycryptodome-3.20.0\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.19.5\n",
            "    Uninstalling protobuf-3.19.5:\n",
            "      Successfully uninstalled protobuf-3.19.5\n",
            "  Attempting uninstall: netaddr\n",
            "    Found existing installation: netaddr 1.2.1\n",
            "    Uninstalling netaddr-1.2.1:\n",
            "      Successfully uninstalled netaddr-1.2.1\n",
            "  Attempting uninstall: multidict\n",
            "    Found existing installation: multidict 6.0.5\n",
            "    Uninstalling multidict-6.0.5:\n",
            "      Successfully uninstalled multidict-6.0.5\n",
            "  Attempting uninstall: lru-dict\n",
            "    Found existing installation: lru-dict 1.3.0\n",
            "    Uninstalling lru-dict-1.3.0:\n",
            "      Successfully uninstalled lru-dict-1.3.0\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.6\n",
            "    Uninstalling idna-3.6:\n",
            "      Successfully uninstalled idna-3.6\n",
            "  Attempting uninstall: hexbytes\n",
            "    Found existing installation: hexbytes 0.3.1\n",
            "    Uninstalling hexbytes-0.3.1:\n",
            "      Successfully uninstalled hexbytes-0.3.1\n",
            "  Attempting uninstall: frozenlist\n",
            "    Found existing installation: frozenlist 1.4.1\n",
            "    Uninstalling frozenlist-1.4.1:\n",
            "      Successfully uninstalled frozenlist-1.4.1\n",
            "  Attempting uninstall: eth-typing\n",
            "    Found existing installation: eth-typing 4.0.0\n",
            "    Uninstalling eth-typing-4.0.0:\n",
            "      Successfully uninstalled eth-typing-4.0.0\n",
            "  Attempting uninstall: eth-hash\n",
            "    Found existing installation: eth-hash 0.6.0\n",
            "    Uninstalling eth-hash-0.6.0:\n",
            "      Successfully uninstalled eth-hash-0.6.0\n",
            "  Attempting uninstall: charset-normalizer\n",
            "    Found existing installation: charset-normalizer 3.3.2\n",
            "    Uninstalling charset-normalizer-3.3.2:\n",
            "      Successfully uninstalled charset-normalizer-3.3.2\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2024.2.2\n",
            "    Uninstalling certifi-2024.2.2:\n",
            "      Successfully uninstalled certifi-2024.2.2\n",
            "  Attempting uninstall: base58\n",
            "    Found existing installation: base58 2.1.1\n",
            "    Uninstalling base58-2.1.1:\n",
            "      Successfully uninstalled base58-2.1.1\n",
            "  Attempting uninstall: attrs\n",
            "    Found existing installation: attrs 23.2.0\n",
            "    Uninstalling attrs-23.2.0:\n",
            "      Successfully uninstalled attrs-23.2.0\n",
            "  Attempting uninstall: async-timeout\n",
            "    Found existing installation: async-timeout 4.0.3\n",
            "    Uninstalling async-timeout-4.0.3:\n",
            "      Successfully uninstalled async-timeout-4.0.3\n",
            "  Attempting uninstall: yarl\n",
            "    Found existing installation: yarl 1.9.4\n",
            "    Uninstalling yarl-1.9.4:\n",
            "      Successfully uninstalled yarl-1.9.4\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.31.0\n",
            "    Uninstalling requests-2.31.0:\n",
            "      Successfully uninstalled requests-2.31.0\n",
            "  Attempting uninstall: parsimonious\n",
            "    Found existing installation: parsimonious 0.9.0\n",
            "    Uninstalling parsimonious-0.9.0:\n",
            "      Successfully uninstalled parsimonious-0.9.0\n",
            "  Attempting uninstall: multiaddr\n",
            "    Found existing installation: multiaddr 0.0.9\n",
            "    Uninstalling multiaddr-0.0.9:\n",
            "      Successfully uninstalled multiaddr-0.0.9\n",
            "  Attempting uninstall: jsonschema\n",
            "    Found existing installation: jsonschema 3.2.0\n",
            "    Uninstalling jsonschema-3.2.0:\n",
            "      Successfully uninstalled jsonschema-3.2.0\n",
            "  Attempting uninstall: cytoolz\n",
            "    Found existing installation: cytoolz 0.12.3\n",
            "    Uninstalling cytoolz-0.12.3:\n",
            "      Successfully uninstalled cytoolz-0.12.3\n",
            "  Attempting uninstall: aiosignal\n",
            "    Found existing installation: aiosignal 1.3.1\n",
            "    Uninstalling aiosignal-1.3.1:\n",
            "      Successfully uninstalled aiosignal-1.3.1\n",
            "  Attempting uninstall: ipfshttpclient\n",
            "    Found existing installation: ipfshttpclient 0.8.0a2\n",
            "    Uninstalling ipfshttpclient-0.8.0a2:\n",
            "      Successfully uninstalled ipfshttpclient-0.8.0a2\n",
            "  Attempting uninstall: eth-utils\n",
            "    Found existing installation: eth-utils 3.0.0\n",
            "    Uninstalling eth-utils-3.0.0:\n",
            "      Successfully uninstalled eth-utils-3.0.0\n",
            "  Attempting uninstall: aiohttp\n",
            "    Found existing installation: aiohttp 3.9.3\n",
            "    Uninstalling aiohttp-3.9.3:\n",
            "      Successfully uninstalled aiohttp-3.9.3\n",
            "  Attempting uninstall: rlp\n",
            "    Found existing installation: rlp 4.0.0\n",
            "    Uninstalling rlp-4.0.0:\n",
            "      Successfully uninstalled rlp-4.0.0\n",
            "  Attempting uninstall: eth-keys\n",
            "    Found existing installation: eth-keys 0.5.0\n",
            "    Uninstalling eth-keys-0.5.0:\n",
            "      Successfully uninstalled eth-keys-0.5.0\n",
            "  Attempting uninstall: eth-abi\n",
            "    Found existing installation: eth_abi 5.0.0\n",
            "    Uninstalling eth_abi-5.0.0:\n",
            "      Successfully uninstalled eth_abi-5.0.0\n",
            "  Attempting uninstall: eth-rlp\n",
            "    Found existing installation: eth-rlp 1.0.1\n",
            "    Uninstalling eth-rlp-1.0.1:\n",
            "      Successfully uninstalled eth-rlp-1.0.1\n",
            "  Attempting uninstall: eth-keyfile\n",
            "    Found existing installation: eth-keyfile 0.7.0\n",
            "    Uninstalling eth-keyfile-0.7.0:\n",
            "      Successfully uninstalled eth-keyfile-0.7.0\n",
            "  Attempting uninstall: eth-account\n",
            "    Found existing installation: eth-account 0.11.0\n",
            "    Uninstalling eth-account-0.11.0:\n",
            "      Successfully uninstalled eth-account-0.11.0\n",
            "  Attempting uninstall: web3\n",
            "    Found existing installation: web3 5.31.4\n",
            "    Uninstalling web3-5.31.4:\n",
            "      Successfully uninstalled web3-5.31.4\n",
            "Successfully installed aiohttp-3.9.3 aiosignal-1.3.1 async-timeout-4.0.3 attrs-23.2.0 base58-2.1.1 bitarray-2.9.2 certifi-2024.2.2 charset-normalizer-3.3.2 cytoolz-0.12.3 eth-abi-2.2.0 eth-account-0.5.9 eth-hash-0.6.0 eth-keyfile-0.5.1 eth-keys-0.3.4 eth-rlp-0.2.1 eth-typing-2.3.0 eth-utils-1.9.5 frozenlist-1.4.1 hexbytes-0.3.1 idna-3.6 ipfshttpclient-0.8.0a2 jsonschema-3.2.0 lru-dict-1.3.0 multiaddr-0.0.9 multidict-6.0.5 netaddr-1.2.1 parsimonious-0.8.1 protobuf-3.19.5 pycryptodome-3.20.0 pyrsistent-0.20.0 pywin32-306 requests-2.31.0 rlp-2.0.1 setuptools-69.1.0 six-1.16.0 toolz-0.12.1 urllib3-2.2.1 varint-1.0.2 web3-5.31.4 websockets-9.1 yarl-1.9.4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "eth-tester 0.10.0b2 requires eth-abi>=3.0.1, but you have eth-abi 2.2.0 which is incompatible.\n",
            "eth-tester 0.10.0b2 requires eth-account>=0.6.0, but you have eth-account 0.5.9 which is incompatible.\n",
            "eth-tester 0.10.0b2 requires eth-keys>=0.4.0, but you have eth-keys 0.3.4 which is incompatible.\n",
            "eth-tester 0.10.0b2 requires eth-utils>=2.0.0, but you have eth-utils 1.9.5 which is incompatible.\n",
            "eth-tester 0.10.0b2 requires rlp>=3.0.0, but you have rlp 2.0.1 which is incompatible.\n"
          ]
        }
      ],
      "source": [
        "!pip install --force-reinstall jsonschema==3.2.0 web3 pycryptodome"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ci5xAstVZUjX",
        "outputId": "bf416f43-0b06-4dd0-e17c-92b50229ec9a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: eth-tester in c:\\users\\alexd\\source\\repos\\rugpulldetection\\.venv\\lib\\site-packages (0.10.0b2)\n",
            "Collecting eth-abi>=3.0.1 (from eth-tester)\n",
            "  Using cached eth_abi-5.0.0-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting eth-account>=0.6.0 (from eth-tester)\n",
            "  Using cached eth_account-0.11.0-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting eth-keys>=0.4.0 (from eth-tester)\n",
            "  Using cached eth_keys-0.5.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting eth-utils>=2.0.0 (from eth-tester)\n",
            "  Using cached eth_utils-3.0.0-py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting rlp>=3.0.0 (from eth-tester)\n",
            "  Using cached rlp-4.0.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: semantic-version>=2.6.0 in c:\\users\\alexd\\source\\repos\\rugpulldetection\\.venv\\lib\\site-packages (from eth-tester) (2.10.0)\n",
            "Collecting eth-typing>=3.0.0 (from eth-abi>=3.0.1->eth-tester)\n",
            "  Using cached eth_typing-4.0.0-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting parsimonious<0.10.0,>=0.9.0 (from eth-abi>=3.0.1->eth-tester)\n",
            "  Using cached parsimonious-0.9.0-py3-none-any.whl\n",
            "Requirement already satisfied: bitarray>=2.4.0 in c:\\users\\alexd\\source\\repos\\rugpulldetection\\.venv\\lib\\site-packages (from eth-account>=0.6.0->eth-tester) (2.9.2)\n",
            "Collecting eth-keyfile>=0.6.0 (from eth-account>=0.6.0->eth-tester)\n",
            "  Using cached eth_keyfile-0.7.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting eth-rlp>=0.3.0 (from eth-account>=0.6.0->eth-tester)\n",
            "  Using cached eth_rlp-1.0.1-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: hexbytes<0.4.0,>=0.1.0 in c:\\users\\alexd\\source\\repos\\rugpulldetection\\.venv\\lib\\site-packages (from eth-account>=0.6.0->eth-tester) (0.3.1)\n",
            "Requirement already satisfied: eth-hash>=0.3.1 in c:\\users\\alexd\\source\\repos\\rugpulldetection\\.venv\\lib\\site-packages (from eth-utils>=2.0.0->eth-tester) (0.6.0)\n",
            "Requirement already satisfied: cytoolz>=0.10.1 in c:\\users\\alexd\\source\\repos\\rugpulldetection\\.venv\\lib\\site-packages (from eth-utils>=2.0.0->eth-tester) (0.12.3)\n",
            "Requirement already satisfied: toolz>=0.8.0 in c:\\users\\alexd\\source\\repos\\rugpulldetection\\.venv\\lib\\site-packages (from cytoolz>=0.10.1->eth-utils>=2.0.0->eth-tester) (0.12.1)\n",
            "Requirement already satisfied: pycryptodome<4,>=3.6.6 in c:\\users\\alexd\\source\\repos\\rugpulldetection\\.venv\\lib\\site-packages (from eth-keyfile>=0.6.0->eth-account>=0.6.0->eth-tester) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.0.1 in c:\\users\\alexd\\source\\repos\\rugpulldetection\\.venv\\lib\\site-packages (from eth-rlp>=0.3.0->eth-account>=0.6.0->eth-tester) (4.9.0)\n",
            "Requirement already satisfied: regex>=2022.3.15 in c:\\users\\alexd\\source\\repos\\rugpulldetection\\.venv\\lib\\site-packages (from parsimonious<0.10.0,>=0.9.0->eth-abi>=3.0.1->eth-tester) (2023.12.25)\n",
            "Using cached eth_abi-5.0.0-py3-none-any.whl (28 kB)\n",
            "Using cached eth_account-0.11.0-py3-none-any.whl (108 kB)\n",
            "Using cached eth_keys-0.5.0-py3-none-any.whl (21 kB)\n",
            "Using cached eth_utils-3.0.0-py3-none-any.whl (77 kB)\n",
            "Using cached rlp-4.0.0-py3-none-any.whl (20 kB)\n",
            "Using cached eth_keyfile-0.7.0-py3-none-any.whl (7.4 kB)\n",
            "Using cached eth_rlp-1.0.1-py3-none-any.whl (4.9 kB)\n",
            "Using cached eth_typing-4.0.0-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: parsimonious, eth-typing, eth-utils, rlp, eth-keys, eth-abi, eth-rlp, eth-keyfile, eth-account\n",
            "  Attempting uninstall: parsimonious\n",
            "    Found existing installation: parsimonious 0.8.1\n",
            "    Uninstalling parsimonious-0.8.1:\n",
            "      Successfully uninstalled parsimonious-0.8.1\n",
            "  Attempting uninstall: eth-typing\n",
            "    Found existing installation: eth-typing 2.3.0\n",
            "    Uninstalling eth-typing-2.3.0:\n",
            "      Successfully uninstalled eth-typing-2.3.0\n",
            "  Attempting uninstall: eth-utils\n",
            "    Found existing installation: eth-utils 1.9.5\n",
            "    Uninstalling eth-utils-1.9.5:\n",
            "      Successfully uninstalled eth-utils-1.9.5\n",
            "  Attempting uninstall: rlp\n",
            "    Found existing installation: rlp 2.0.1\n",
            "    Uninstalling rlp-2.0.1:\n",
            "      Successfully uninstalled rlp-2.0.1\n",
            "  Attempting uninstall: eth-keys\n",
            "    Found existing installation: eth-keys 0.3.4\n",
            "    Uninstalling eth-keys-0.3.4:\n",
            "      Successfully uninstalled eth-keys-0.3.4\n",
            "  Attempting uninstall: eth-abi\n",
            "    Found existing installation: eth-abi 2.2.0\n",
            "    Uninstalling eth-abi-2.2.0:\n",
            "      Successfully uninstalled eth-abi-2.2.0\n",
            "  Attempting uninstall: eth-rlp\n",
            "    Found existing installation: eth-rlp 0.2.1\n",
            "    Uninstalling eth-rlp-0.2.1:\n",
            "      Successfully uninstalled eth-rlp-0.2.1\n",
            "  Attempting uninstall: eth-keyfile\n",
            "    Found existing installation: eth-keyfile 0.5.1\n",
            "    Uninstalling eth-keyfile-0.5.1:\n",
            "      Successfully uninstalled eth-keyfile-0.5.1\n",
            "  Attempting uninstall: eth-account\n",
            "    Found existing installation: eth-account 0.5.9\n",
            "    Uninstalling eth-account-0.5.9:\n",
            "      Successfully uninstalled eth-account-0.5.9\n",
            "Successfully installed eth-abi-5.0.0 eth-account-0.11.0 eth-keyfile-0.7.0 eth-keys-0.5.0 eth-rlp-1.0.1 eth-typing-4.0.0 eth-utils-3.0.0 parsimonious-0.9.0 rlp-4.0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "web3 5.31.4 requires eth-abi<3.0.0,>=2.2.0, but you have eth-abi 5.0.0 which is incompatible.\n",
            "web3 5.31.4 requires eth-account<0.6.0,>=0.5.9, but you have eth-account 0.11.0 which is incompatible.\n",
            "web3 5.31.4 requires eth-rlp<0.3, but you have eth-rlp 1.0.1 which is incompatible.\n",
            "web3 5.31.4 requires eth-typing<3.0.0,>=2.0.0, but you have eth-typing 4.0.0 which is incompatible.\n",
            "web3 5.31.4 requires eth-utils<2.0.0,>=1.9.5, but you have eth-utils 3.0.0 which is incompatible.\n"
          ]
        }
      ],
      "source": [
        "!pip install eth-tester"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZiB5wTjxZa_7",
        "outputId": "0e6cfcfa-4a4d-4d23-d9b9-e58659746553"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting typing_extensions"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "web3 5.31.4 requires eth-abi<3.0.0,>=2.2.0, but you have eth-abi 5.0.0 which is incompatible.\n",
            "web3 5.31.4 requires eth-account<0.6.0,>=0.5.9, but you have eth-account 0.11.0 which is incompatible.\n",
            "web3 5.31.4 requires eth-rlp<0.3, but you have eth-rlp 1.0.1 which is incompatible.\n",
            "web3 5.31.4 requires eth-typing<3.0.0,>=2.0.0, but you have eth-typing 4.0.0 which is incompatible.\n",
            "web3 5.31.4 requires eth-utils<2.0.0,>=1.9.5, but you have eth-utils 3.0.0 which is incompatible.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "  Using cached typing_extensions-4.9.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Using cached typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
            "Installing collected packages: typing_extensions\n",
            "  Attempting uninstall: typing_extensions\n",
            "    Found existing installation: typing_extensions 4.9.0\n",
            "    Uninstalling typing_extensions-4.9.0:\n",
            "      Successfully uninstalled typing_extensions-4.9.0\n",
            "Successfully installed typing_extensions-4.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install --force-reinstall typing_extensions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EF1UP_t986lg",
        "outputId": "066f93b8-b33a-4950-b6b9-30447ae711a1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'RugPullDetection' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/T2Project/RugPullDetection.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyTp_50zqQrm"
      },
      "source": [
        "# Shared"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 532
        },
        "id": "-n1HjybSqQSm",
        "outputId": "6930c18e-42e6-4074-a2fb-4231348a10d8"
      },
      "outputs": [],
      "source": [
        "import configparser\n",
        "import subprocess\n",
        "from web3 import Web3\n",
        "\n",
        "class shared:\n",
        "  def init():\n",
        "    global INFURA_URL\n",
        "    global USE_POOL_INFURA\n",
        "    global USE_WALLET_INFURA\n",
        "    global API_KEY\n",
        "    global UNISWAP_FACTORY\n",
        "    global SUSHISWAP_FACTORY\n",
        "    global UNILOCK_ADDRESS\n",
        "    global EXCHANGES\n",
        "    global ETH_ADDRESS\n",
        "    global DEAD_ADDRESS\n",
        "    global WETH\n",
        "    global TOKEN_TOLERANCE\n",
        "    global POOLS\n",
        "    global FAKE_POOL\n",
        "    global MULTICALL_CONTRACT\n",
        "    global TRANSACTION\n",
        "    \n",
        "    global ROOT_FOLDER\n",
        "    global WEEK\n",
        "    global ABI\n",
        "    global ABI_FACTORY\n",
        "    global BLOCKSTUDY\n",
        "    global ABI_POOL\n",
        "    \n",
        "    global web3\n",
        "    global multicall\n",
        "\n",
        "    ROOT_FOLDER        = subprocess.run([\"git\", \"rev-parse\", \"--show-toplevel\"], check=True, universal_newlines=True, stdout=subprocess.PIPE).stdout.strip()\n",
        "    config = configparser.ConfigParser()\n",
        "    config.read(ROOT_FOLDER + \"/config.ini\")\n",
        "\n",
        "    # # Get infura node url and logic booleans\n",
        "    INFURA_URL         = config.get(\"NODE\", \"INFURA_URL\")\n",
        "    USE_POOL_INFURA    = config.getboolean(\"NODE\", \"USE_POOL_INFURA\")\n",
        "    USE_WALLET_INFURA  = config.getboolean(\"NODE\", \"USE_WALLET_INFURA\")\n",
        "    \n",
        "    # Get etherscan keys\n",
        "    API_KEY            = config.get(\"APIS\", \"ETHERSCAN_API_KEY\")\n",
        "\n",
        "    # Get tokens addresses\n",
        "    ETH_ADDRESS        = config.get(\"ADDRESS\", \"ETH_ADDRESS\")\n",
        "    DEAD_ADDRESS       = config.get(\"ADDRESS\", \"DEAD_ADDRESS\")\n",
        "    WETH               = config.get(\"ADDRESS\", \"WETH_ADDRESS\")\n",
        "    MULTICALL_CONTRACT = config.get(\"ADDRESS\", \"MULTICALL_CONTRACT\")\n",
        "\n",
        "    # Get routers addresses\n",
        "    UNISWAP_ADDRESS    = config.get(\"ROUTERS\", \"UNISWAP_ADDRESS\")\n",
        "    UNILOCK_ADDRESS    = config.get(\"ADDRESS\", \"UNILOCK_ADDRESS\")\n",
        "    SUSHISWAP_ADDRESS  = config.get(\"ROUTERS\", \"SUSHISWAP_ADDRESS\")\n",
        "    FAKE_POOL          = config.get(\"POOLS\", \"FAKE_POOL\")\n",
        "\n",
        "    # Get pools factories\n",
        "    UNISWAP_FACTORY    = config.get(\"FACTORIES\", \"UNISWAP_FACTORY\")\n",
        "\n",
        "    #Get log hashes\n",
        "    TRANSACTION        = config.get(\"LOG_HASHES\",\"TRANSACTION\")\n",
        "\n",
        "    EXCHANGES          = {\n",
        "        \"uniswap\": UNISWAP_ADDRESS.lower(),\n",
        "        \"sushiswap\": SUSHISWAP_ADDRESS.lower()\n",
        "    }\n",
        "    \n",
        "    TOKEN_TOLERANCE    = float(config.get(\"THRESHOLDS\", \"TOKEN_TOLERANCE\"))\n",
        "    # Define other globals not saved in config.ini\n",
        "    WEEK               = 4*60*24*7\n",
        "    ABI                = open(ROOT_FOLDER + \"/ABIs/normal_token_abi.txt\").read()\n",
        "    ABI_FACTORY        = open(ROOT_FOLDER + \"/ABIs/factory_abi.txt\").read()\n",
        "    ABI_POOL           = open(ROOT_FOLDER + \"/ABIs/abi_pool.txt\").read()\n",
        "    # Define global objects\n",
        "    BLOCKSTUDY        =  13152303\n",
        "\n",
        "    web3 = Web3(Web3.HTTPProvider(INFURA_URL))\n",
        "    # multicall =  Multicall(web3.eth)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZSUkPhnp6qa"
      },
      "source": [
        "# Utility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "RiSkOMfBp5ZG"
      },
      "outputs": [],
      "source": [
        "from Crypto.Hash import keccak\n",
        "\n",
        "\n",
        "def obtain_hash_event(event: str) -> str:\n",
        "    k = keccak.new(digest_bits=256)\n",
        "    k.update(bytes(event, encoding='utf-8'))\n",
        "    return '0x'+k.hexdigest()\n",
        "\n",
        "\n",
        "def obtain_events_from_abi(abi: dict) -> list:\n",
        "    events = []\n",
        "    for function in abi:\n",
        "        if function['type'] == 'event':\n",
        "            event = function['name'] + '('\n",
        "            for cont, element in enumerate(function['inputs']):\n",
        "                if cont == 0:\n",
        "                    event += element['type']\n",
        "\n",
        "                else:\n",
        "                    event += ','+element['type']\n",
        "\n",
        "                cont += 1\n",
        "\n",
        "            event  += ')'\n",
        "            events.append(event)\n",
        "\n",
        "    return events"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "-pi5YNJnpx4g"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from web3.datastructures import AttributeDict\n",
        "from hexbytes import HexBytes\n",
        "import sys\n",
        "sys.path.append(\"../\")\n",
        "shared.init()\n",
        "\n",
        "\n",
        "def get_rpc_response(method, list_params=[]):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    method: str\n",
        "        Indicates node method.\n",
        "    list_params: List[Dict[str, Any]]\n",
        "        List of request parameters.\n",
        "    Returns\n",
        "    -------\n",
        "    args_event: AttributeDict\n",
        "        Change number basis.\n",
        "    Example\n",
        "    -------\n",
        "        If we want token transfers of 0xa150Db9b1Fa65b44799d4dD949D922c0a33Ee606\n",
        "        between blocks [11000000, 11025824] then:\n",
        "        method: 'eth_getLogs'\n",
        "        list_params: [[{'address': '0xa150Db9b1Fa65b44799d4dD949D922c0a33Ee606',\n",
        "                    'fromBlock': '0xa7d8c0', 'toBlock': '0xa83da0',\n",
        "                    'topics': ['0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef']}]]\n",
        "    \"\"\"\n",
        "    url = shared.INFURA_URL\n",
        "    list_params = list_params or []\n",
        "    data = [{\"jsonrpc\": \"2.0\", \"method\": method, \"params\": params, \"id\": 1} for params in list_params]\n",
        "    headers = {\"Content-Type\": \"application/json\"}\n",
        "    response = requests.post(url, headers=headers, json=data)\n",
        "    return response.json()\n",
        "\n",
        "\n",
        "def change_log_dict(log_dict):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    log_dict: AttributeDict\n",
        "        Decoded logs.\n",
        "    Returns\n",
        "    -------\n",
        "    args_event: AttributeDict\n",
        "        Change number basis.\n",
        "    \"\"\"\n",
        "    dictionary = log_dict.copy()\n",
        "    dictionary['blockHash'] = HexBytes(dictionary['blockHash'])\n",
        "    dictionary['blockNumber'] = int(dictionary['blockNumber'], 16)\n",
        "    dictionary['logIndex'] = int(dictionary['logIndex'], 16)\n",
        "    for i in range(len(dictionary['topics'])):\n",
        "        dictionary['topics'][i] = HexBytes(dictionary['topics'][i])\n",
        "    dictionary['transactionHash'] = HexBytes(dictionary['transactionHash'])\n",
        "    dictionary['transactionIndex'] = int(dictionary['transactionIndex'], 16)\n",
        "    return AttributeDict(dictionary)\n",
        "\n",
        "\n",
        "def clean_logs(contract, myevent, log):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    contract: web3.eth.contract\n",
        "        Contract that contains the event.\n",
        "    myevent: str\n",
        "        string with event name.\n",
        "    log: List[AttributeDict]\n",
        "        List containing raw node response.\n",
        "    Returns\n",
        "    -------\n",
        "    args_event: AttributeDict\n",
        "        Decoded logs.\n",
        "    \"\"\"\n",
        "    log_dict = AttributeDict({'logs': log})\n",
        "    eval_string = 'contract.events.{}().processReceipt({})'.format(myevent, log_dict)\n",
        "    args_event = eval(eval_string)[0]\n",
        "    return args_event\n",
        "\n",
        "\n",
        "def get_logs(contract, myevent, hash_create, from_block, to_block, number_batches):\n",
        "    \"\"\"\n",
        "    Get event logs using recursion.\n",
        "    Parameters\n",
        "    ----------\n",
        "    contract: web3.eth.contract\n",
        "        Contract that contains the event.\n",
        "    myevent: str\n",
        "        string with event name.\n",
        "    hash_create: str\n",
        "        hash of the event.\n",
        "    from_block: int\n",
        "        Starting block.\n",
        "    to_block: int\n",
        "        Ending block.\n",
        "    number_batches: int\n",
        "        infura returns just 10k logs each call, therefore we need to split time series into batches.\n",
        "    Returns\n",
        "    -------\n",
        "    events_clean: list\n",
        "        List with all clean logs.\n",
        "    \"\"\"\n",
        "\n",
        "    events_clean = []\n",
        "    block_list = [int(from_block + i * (to_block - from_block) / number_batches) for i in range(0, number_batches)] + [\n",
        "        to_block]\n",
        "\n",
        "    block_list[0] -= 1\n",
        "    list_params = [[{\"address\": contract.address,\n",
        "                     \"fromBlock\": hex(block_list[i - 1] + 1),\n",
        "                     \"toBlock\": hex(block_list[i]),\n",
        "                     \"topics\": [hash_create]}] for i in range(1, number_batches + 1)]\n",
        "\n",
        "    logs = get_rpc_response(\"eth_getLogs\", list_params)\n",
        "    for j, log in enumerate(logs):\n",
        "        if list(log.keys())[-1] == \"result\":\n",
        "            for event in log['result']:\n",
        "                log_dict = change_log_dict(event)\n",
        "                events_clean += [clean_logs(contract, myevent, [log_dict])]\n",
        "        else:\n",
        "            events_clean += get_logs(contract, myevent, hash_create, int(list_params[j][0][\"fromBlock\"], 16),\n",
        "                                     int(list_params[j][0][\"toBlock\"], 16), 10)\n",
        "    return events_clean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "Z5wggwdOWrqR"
      },
      "outputs": [],
      "source": [
        "from web3 import Web3, HTTPProvider\n",
        "shared.init()\n",
        "from itertools import islice\n",
        "\n",
        "\n",
        "def connect_to_web3():\n",
        "    \"\"\"\n",
        "    Connect to Web3 server.\n",
        "    Parameters\n",
        "    ----------\n",
        "    Returns\n",
        "    -------\n",
        "    res: bool\n",
        "        True if connection was succeeded, otherwise False\n",
        "    web3: Web3 Object\n",
        "    \"\"\"\n",
        "    web3 = Web3(HTTPProvider('https://mainnet.infura.io/v3/d6243bb783b44485ad6636b6c3411377'))\n",
        "    res = web3.isConnected()\n",
        "    return res, web3\n",
        "\n",
        "\n",
        "def split_chunks(data, n_elements):\n",
        "    \"\"\"\n",
        "    Splitting the calls to aggregate them properly.\n",
        "    Parameters\n",
        "    ----------\n",
        "    data: np.array\n",
        "        Containing calls\n",
        "    n_elements: int\n",
        "        Calls we want to aggregate\n",
        "    Returns\n",
        "    -------\n",
        "    chunks: list\n",
        "        Calls in chunks\n",
        "    \"\"\"\n",
        "\n",
        "    chunks = []\n",
        "    n = len(data)\n",
        "    \n",
        "    if n % n_elements != 0:\n",
        "        n_chunks = int(n/n_elements)+1\n",
        "\n",
        "    else:\n",
        "        n_chunks = int(n/n_elements)\n",
        "            \n",
        "    for  i in range(n_chunks-1):\n",
        "        chunk = data[i*n_elements:(i+1)*n_elements]\n",
        "        chunks.append(chunk)\n",
        "\n",
        "    chunks.append(data[(n_chunks-1)*n_elements:]) \n",
        "    return chunks\n",
        "\n",
        "\n",
        "def chunks(data, SIZE=10000):\n",
        "    it = iter(data)\n",
        "    for i in range(0, len(data), SIZE):\n",
        "        yield {k:data[k] for k in islice(it, SIZE)}\n",
        "\n",
        "\n",
        "def get_pools(dex, factory): #v2 or sushi\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    dex : str\n",
        "        Choose the DEX you want to get the pools from\n",
        "    factory : web3.eth.contract\n",
        "        factory get_contracts of dex\n",
        "    Returns\n",
        "    -------\n",
        "    pool_dic: Dict[str, float]\n",
        "        pool dictionary with the attributes 'address','dex','token0','token1','reserves0','reserves1','creation'\n",
        "    \"\"\"\n",
        "\n",
        "    hash_create = '0x0d3648bd0f6ba80134a33ba9275ac585d9d315f0ad8355cddefde31afa28d0e9'\n",
        "    if dex == 'uniswap_v2':\n",
        "        from_block = 10008355\n",
        "        to_block = shared.BLOCKSTUDY\n",
        "        number_batches = 10\n",
        "        pools = get_logs(factory, 'PairCreated', hash_create, from_block, to_block, number_batches)\n",
        "    \n",
        "    if dex == 'sushiswap':\n",
        "        from_block = 10822038\n",
        "        to_block = shared.BLOCKSTUDY\n",
        "        number_batches = 10\n",
        "        pools = get_logs(factory, 'PairCreated', hash_create, from_block, to_block, number_batches)\n",
        "\n",
        "    pool_dic = {}\n",
        "    tokens = {}\n",
        "    for pool in pools:\n",
        "        pool_address = pool.args.pair\n",
        "        token0 = pool.args.token0\n",
        "        token1 = pool.args.token1\n",
        "        tokens.update({token0:None})\n",
        "        tokens.update({token1:None})\n",
        "        pool_dic.update({pool_address:{ 'address':pool_address,\n",
        "                                        'dex':dex,\n",
        "                                        'token0':token0,\n",
        "                                        'token1':token1,\n",
        "                                        'reserves0':None,\n",
        "                                        'reserves1':None,\n",
        "                                        'creation':pool.blockNumber}})   \n",
        "\n",
        "    return pool_dic, tokens\n",
        "\n",
        "\n",
        "def events_to_json(events):\n",
        "    json_events = []\n",
        "    for event in events:\n",
        "        event_dict = dict(event)\n",
        "        # event_dict.pop('transactionHash')\n",
        "        event_dict['transactionHash'] = event_dict['transactionHash'].hex()\n",
        "        event_dict.pop('blockHash')\n",
        "        event_dict['args'] = dict(event.args)\n",
        "        json_events.append(event_dict)\n",
        "       \n",
        "    return json_events"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "spXBjR_L86c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokens and Pools downloaded!\n"
          ]
        }
      ],
      "source": [
        "# get_tokens&pools.py\n",
        "\n",
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "import shared\n",
        "shared.init()\n",
        "def get_token_and_pools(out_path, dex='uniswap_v2'):\n",
        "    \"\"\"\n",
        "    Get tokens and pools from sushiswap or uniswap_v2.\n",
        "    Parameters\n",
        "    ----------\n",
        "    out_path : str\n",
        "        Path to output directory.\n",
        "    dex : str\n",
        "        sushiswap or uniswap_v2 are currently allowed.\n",
        "    \"\"\"\n",
        "\n",
        "    factory = shared.web3.eth.contract(shared.UNISWAP_FACTORY, abi=shared.ABI_FACTORY)\n",
        "    pool_dic, tokens = get_pools(dex, factory)\n",
        "    pd.DataFrame(tokens.keys(), columns=[\"token_address\"]).to_csv(f\"{out_path}/tokens.csv\", index=False)\n",
        "\n",
        "    with open(f\"{out_path}/pool_dict.json\", \"w\") as outfile:\n",
        "        json.dump(pool_dic, outfile)\n",
        "\n",
        "    inverted_pool_dict = dict()\n",
        "    for pool in pool_dic.keys():\n",
        "        try:\n",
        "            inverted_pool_dict[pool_dic[pool]['token0']].append(pool_dic[pool])\n",
        "        except:\n",
        "            inverted_pool_dict[pool_dic[pool]['token0']] = [pool_dic[pool]]\n",
        "        try:\n",
        "            inverted_pool_dict[pool_dic[pool]['token1']].append(pool_dic[pool])\n",
        "        except:\n",
        "            inverted_pool_dict[pool_dic[pool]['token1']] = [pool_dic[pool]]\n",
        "\n",
        "    with open(f\"{out_path}/pools_of_token.json\", \"w\") as outfile:\n",
        "        json.dump(inverted_pool_dict, outfile)\n",
        "\n",
        "    print('Tokens and Pools downloaded!')\n",
        "\n",
        "\n",
        "get_token_and_pools(\"./data\", dex='uniswap_v2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "FNas8lEDo9rq"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "shared.init()\n",
        "\n",
        "def get_pool_events(event_name, hashed_event, pool_address, out_path, start_block, end_block):\n",
        "    \"\"\"\n",
        "    Get pool logs for a given pool and period.\n",
        "    This function saves the events as a json in out_path.\n",
        "    Parameters\n",
        "    ----------\n",
        "    event_name: str\n",
        "        Pool event in string format (example: Mint)\n",
        "    hashed_event: str\n",
        "        The hashed event. Use obtain_hash_event() if necessary.\n",
        "    pool_address : str\n",
        "        Token address.\n",
        "    out_path : str\n",
        "        Path to output directory.\n",
        "    start_block: float\n",
        "        Starting block.\n",
        "    end_block: float\n",
        "        Ending block.\n",
        "    \"\"\"\n",
        "\n",
        "    pool = shared.web3.eth.contract(pool_address, abi=shared.ABI_POOL)\n",
        "    try:\n",
        "        events = get_logs(pool, event_name, hashed_event, start_block, end_block, number_batches=10)\n",
        "    except Exception as err:\n",
        "        print(f\"Exception occured: {err}\")\n",
        "        return\n",
        "\n",
        "    json_events = events_to_json(events)\n",
        "    with open(f'{out_path}/{pool_address}.json', 'w+') as f:\n",
        "        json.dump(json_events, f)\n",
        "    f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "QDaZj-PEpJ-a"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "\n",
        "\n",
        "def _obtain_tx_creation(token_address):\n",
        "    \"\"\"\n",
        "    Gets token tx creation hash via web scrapping.\n",
        "    Parameters\n",
        "    ----------\n",
        "    token_address: str\n",
        "        string corresponding to token address\n",
        "    Returns\n",
        "    -------\n",
        "        tx_creation hash if success, \"Not found\" otherwise.\n",
        "    \"\"\"\n",
        "\n",
        "    tx_hash_creation = \"Not found\"\n",
        "    headers = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/'\n",
        "                             '537.36 (KHTML, like Gecko) Chrome/47.0.2526.106 Safari/537.36'}\n",
        "\n",
        "    r = requests.get(f'https://etherscan.io/address/{token_address}', headers=headers)\n",
        "    soup = BeautifulSoup(r.content, 'html.parser')\n",
        "    soup2 = soup.find_all('div', 'col-md-8')\n",
        "    for element in soup2:\n",
        "        if \"Creator Txn Hash\" in str(element):\n",
        "            for element2 in str(element).split():\n",
        "                if 'href' and 'tx' in str(element2):\n",
        "                    tx_hash_creation = str(element2[10:]).replace('\"', '')\n",
        "    if tx_hash_creation != \"Not found\":\n",
        "        return tx_hash_creation\n",
        "    return \"Not found\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "Hm5XlzwZpMI1"
      },
      "outputs": [],
      "source": [
        "shared.init()\n",
        "def get_decimal_token(token_address):\n",
        "    \"\"\"\n",
        "    Gets token decimal given a token address.\n",
        "    Parameters\n",
        "    ----------\n",
        "    token_address: str\n",
        "        String containing token address.\n",
        "    Returns\n",
        "    -------\n",
        "    decimal: int\n",
        "        Int corresponding to token decimal.\n",
        "    \"\"\"\n",
        "\n",
        "    contract = shared.web3.eth.contract(token_address, abi=shared.ABI)\n",
        "    try:\n",
        "        decimal = contract.functions.decimals().call()\n",
        "    except:\n",
        "        decimals = 0\n",
        "        print(f\"Error getting decimals for {token_address}\")\n",
        "\n",
        "    return decimals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "AJW8kc99pX2D"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import requests\n",
        "shared.init()\n",
        "\n",
        "def get_source_code(token_address, out_path):\n",
        "    \"\"\"\n",
        "    Obtains token source code/abi and saves in json format.\n",
        "    Parameters\n",
        "    ----------\n",
        "    token_address: str\n",
        "        token address in checksum format.\n",
        "    out_path : str\n",
        "        Path to output directory.\n",
        "    \"\"\"\n",
        "\n",
        "    source_code_endpoint = \"https://api.etherscan.io/api?\" \\\n",
        "                           \"module=contract\" \\\n",
        "                           \"&action=getsourcecode\" \\\n",
        "                           f\"&address={token_address}\" \\\n",
        "                           f\"&apikey={shared.API_KEY}\"\n",
        "    source_code = json.loads(requests.get(source_code_endpoint).text)['result']\n",
        "\n",
        "    with open(f\"{out_path}/{token_address}.json\", \"w\") as outfile:\n",
        "        json.dump(source_code, outfile)\n",
        "    outfile.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "x5xVc9pcpgpp"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from web3 import Web3\n",
        "shared.init()\n",
        "def get_transfers(token_address, out_path, start_block, end_block, decimal=18):\n",
        "    \"\"\"\n",
        "    Get transfer logs for a given token and period.\n",
        "    This function saves transfers as a csv in out_path.\n",
        "    Parameters\n",
        "    ----------\n",
        "    token_address : str\n",
        "        Token address.\n",
        "    out_path : str\n",
        "        Path to output directory.\n",
        "    decimal: float\n",
        "        Token decimal (usually 18).\n",
        "    start_block: int\n",
        "        Starting block.\n",
        "    end_block: int\n",
        "        Ending block.\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialise contract objects and get the transactions.\n",
        "    try:\n",
        "        contract = shared.web3.eth.contract(Web3.toChecksumAddress(token_address), abi=shared.ABI)\n",
        "        transfers = get_logs(contract, \"Transfer\", hash_log, start_block, end_block, number_batches=1)\n",
        "    except Exception as err:\n",
        "        print(f\"Exception occured: {err}\")\n",
        "        return\n",
        "\n",
        "    # Save txs in a Dataframe.\n",
        "    txs = [[transaction['transactionHash'].hex(), transaction[\"blockNumber\"], transaction[\"args\"]['from'],\n",
        "            transaction[\"args\"]['to'], transaction[\"args\"]['value'] / 10 ** decimal] for transaction in transfers]\n",
        "    transfers = pd.DataFrame(txs, columns=[\"transactionHash\", \"block_number\", \"from\", \"to\", \"value\"])\n",
        "    transfers.to_csv(out_path + \"/\" + token_address + \".csv\", index=False)\n",
        "    return\n",
        "\n",
        "\n",
        "hash_log = obtain_hash_event('Transfer(address,address,uint256)')\n",
        "get_transfers('0xB4FBF271143F4FBf7B91A5ded31805e42b2208d6', './', 11000000, 11025824) # example"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNpR/kynqPi1YNYhV8d4ON9",
      "include_colab_link": true,
      "name": "Do not rug on me.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
